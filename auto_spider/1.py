import requests
import csv
import time
import json

def get_new_cookie():
    """è·å–æ–°cookieçš„æ­¥éª¤"""
    print("=" * 60)
    print("ğŸ”‘ éœ€è¦æ›´æ–°Cookieï¼")
    print("=" * 60)
    print("è·å–æ–°Cookieçš„æ­¥éª¤ï¼š")
    print("1. æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® https://weibo.com")
    print("2. ç™»å½•ä½ çš„å¾®åšè´¦å·")
    print("3. æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…·")
    print("4. åˆ‡æ¢åˆ° Networkï¼ˆç½‘ç»œï¼‰æ ‡ç­¾")
    print("5. åˆ·æ–°å¾®åšé¡µé¢")
    print("6. åœ¨è¯·æ±‚åˆ—è¡¨ä¸­æ‰¾åˆ°ä»»æ„ä¸€ä¸ªè¯·æ±‚ï¼ˆå¦‚ hotflowï¼‰")
    print("7. åœ¨ Request Headersï¼ˆè¯·æ±‚å¤´ï¼‰ä¸­æ‰¾åˆ° Cookie")
    print("8. å¤åˆ¶æ•´ä¸ªCookieå­—ç¬¦ä¸²")
    print("=" * 60)
    return input("è¯·ç²˜è´´æ–°çš„Cookieå€¼: ").strip()

def crawl_with_cookie(weibo_url, cookie=None):
    """ä½¿ç”¨cookieçˆ¬å–"""
    
    # æå–å‚æ•°
    weibo_id = weibo_url.split('id=')[1].split('&')[0] if 'id=' in weibo_url else ''
    user_id = weibo_url.split('uid=')[1].split('&')[0] if 'uid=' in weibo_url else ''
    
    filename = f"å¾®åš{weibo_id}_è¯„è®º.csv"
    
    # å¦‚æœæ²¡æœ‰æä¾›cookieï¼Œå°è¯•ä»æ–‡ä»¶è¯»å–æˆ–è¦æ±‚è¾“å…¥
    if not cookie:
        try:
            with open('weibo_cookie.txt', 'r', encoding='utf-8') as f:
                cookie = f.read().strip()
                print(f"ğŸ“ ä»æ–‡ä»¶è¯»å–cookieï¼Œé•¿åº¦: {len(cookie)} å­—ç¬¦")
        except:
            cookie = get_new_cookie()
            # ä¿å­˜cookieåˆ°æ–‡ä»¶
            with open('weibo_cookie.txt', 'w', encoding='utf-8') as f:
                f.write(cookie)
    
    # è¯·æ±‚å¤´ï¼ˆåŒ…å«cookieï¼‰
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': f'https://weibo.com/{user_id}',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Cookie': cookie,  # å…³é”®ï¼æ·»åŠ cookie
        'x-requested-with': 'XMLHttpRequest',
    }
    
    print(f"ğŸ¯ å¼€å§‹çˆ¬å–å¾®åš {weibo_id}...")
    print(f"ğŸ“ è¾“å‡º: {filename}")
    
    # åˆå§‹åŒ–CSV
    with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
        csv.writer(f).writerow(['id', 'ç”¨æˆ·å', 'è¯„è®ºå†…å®¹', 'ç‚¹èµæ•°', 'æ—¶é—´'])
    
    base_url = "https://weibo.com/ajax/statuses/buildComments"
    params = {
        'is_reload': 1,
        'id': weibo_id,
        'is_show_bulletin': 2,
        'is_mix': 0,
        'count': 20,
        'uid': user_id,
        'fetch_level': 0,
        'locale': 'zh-CN'
    }
    
    max_id = 0
    page = 1
    total = 0
    
    while page <= 100:  # æœ€å¤š100é¡µ
        print(f"ğŸ“„ ç¬¬ {page} é¡µ...")
        
        if max_id:
            params['max_id'] = max_id
        elif 'max_id' in params:
            del params['max_id']
        
        try:
            response = requests.get(base_url, headers=headers, params=params, timeout=15)
            
            print(f"ğŸ“¥ çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code != 200:
                print(f"âŒ è¯·æ±‚å¤±è´¥")
                if response.status_code == 403:
                    print("ğŸ”’ è®¿é—®è¢«æ‹’ç»ï¼Œcookieå¯èƒ½å·²è¿‡æœŸ")
                    # å°è¯•æ›´æ–°cookie
                    new_cookie = get_new_cookie()
                    headers['Cookie'] = new_cookie
                    # ä¿å­˜æ–°cookie
                    with open('weibo_cookie.txt', 'w', encoding='utf-8') as f:
                        f.write(new_cookie)
                    print("ğŸ”„ ä½¿ç”¨æ–°cookieé‡è¯•...")
                    continue
                break
            
            data = response.json()
            
            # æ£€æŸ¥å“åº”å†…å®¹
            if 'ok' in data and 'url' in data:
                print(f"âš ï¸ éœ€è¦ç™»å½•æˆ–é‡å®šå‘: {data.get('url', '')}")
                print("è¯·æ›´æ–°cookieï¼")
                break
            
            if 'data' not in data:
                print(f"âŒ æ— æ•°æ®å­—æ®µ")
                print(f"å“åº”: {data}")
                break
            
            comments = data.get('data', [])
            
            if not comments:
                print("âœ… æ²¡æœ‰æ›´å¤šè¯„è®º")
                break
            
            # ä¿å­˜æ•°æ®
            with open(filename, 'a', encoding='utf-8-sig', newline='') as f:
                writer = csv.writer(f)
                for comment in comments:
                    try:
                        writer.writerow([
                            comment.get('id', ''),
                            comment.get('user', {}).get('screen_name', ''),
                            comment.get('text_raw', '').replace('\n', ' ')[:500],  # é™åˆ¶é•¿åº¦
                            comment.get('like_counts', 0),
                            comment.get('created_at', '')
                        ])
                    except:
                        continue
            
            total += len(comments)
            print(f"âœ… è·å– {len(comments)} æ¡ï¼Œç´¯è®¡ {total} æ¡")
            
            # ä¸‹ä¸€é¡µ
            next_max_id = data.get('max_id', 0)
            if not next_max_id:
                print("âœ… å·²è·å–æ‰€æœ‰è¯„è®º")
                break
            
            max_id = next_max_id
            page += 1
            time.sleep(1.5)
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            break
    
    print(f"ğŸ‰ å®Œæˆï¼å…±è·å– {total} æ¡è¯„è®º")
    return total > 0  # è¿”å›æ˜¯å¦æˆåŠŸ

# æ–¹æ¡ˆ2ï¼šä½¿ç”¨ç®€åŒ–ç‰ˆï¼ˆä¸ä¾èµ–cookieï¼‰
def simple_crawl(weibo_url):
    """ç®€åŒ–ç‰ˆçˆ¬è™«ï¼Œé€‚åˆå…¬å¼€å†…å®¹"""
    print("ğŸš€ ä½¿ç”¨ç®€åŒ–ç‰ˆçˆ¬è™«...")
    
    weibo_id = weibo_url.split('id=')[1].split('&')[0]
    filename = f"å¾®åš{weibo_id}_è¯„è®º.csv"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://weibo.com/',
    }
    
    print(f"æ­£åœ¨çˆ¬å–å¾®åš {weibo_id}...")
    
    # ç›´æ¥ä½¿ç”¨å®Œæ•´URL
    url = weibo_url
    page = 1
    
    while page <= 10:
        print(f"ç¬¬ {page} é¡µ...")
        
        try:
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code != 200:
                print(f"çŠ¶æ€ç : {response.status_code}")
                break
            
            data = response.json()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
            if 'ok' in data and 'url' in data:
                print("ğŸ˜” éœ€è¦ç™»å½•æ‰èƒ½æŸ¥çœ‹æ­¤å¾®åš")
                print("è¯·ä½¿ç”¨å¸¦cookieçš„ç‰ˆæœ¬")
                break
            
            if 'data' not in data:
                print("æ— æ•°æ®")
                break
            
            comments = data.get('data', [])
            
            if not comments:
                print("æ²¡æœ‰æ›´å¤šè¯„è®º")
                break
            
            # ä¿å­˜æ•°æ®
            mode = 'a' if page > 1 else 'w'
            with open(filename, mode, encoding='utf-8-sig', newline='') as f:
                writer = csv.writer(f)
                if page == 1:
                    writer.writerow(['ç”¨æˆ·å', 'å†…å®¹', 'ç‚¹èµ', 'æ—¶é—´'])
                
                for comment in comments:
                    writer.writerow([
                        comment.get('user', {}).get('screen_name', ''),
                        comment.get('text_raw', '')[:200],
                        comment.get('like_counts', 0),
                        comment.get('created_at', '')
                    ])
            
            print(f"è·å– {len(comments)} æ¡è¯„è®º")
            
            # ä¸‹ä¸€é¡µ
            max_id = data.get('max_id', 0)
            if not max_id:
                break
            
            # æ„å»ºä¸‹ä¸€é¡µURL
            if 'max_id=' in url:
                url = url.split('max_id=')[0] + 'max_id=' + str(max_id)
            else:
                url = url + '&max_id=' + str(max_id)
            
            page += 1
            time.sleep(2)
            
        except Exception as e:
            print(f"é”™è¯¯: {e}")
            break
    
    print(f"å®Œæˆï¼æ•°æ®ä¿å­˜åˆ° {filename}")

if __name__ == "__main__":
    print("å¾®åšè¯„è®ºçˆ¬è™« v3.0")
    print("=" * 60)
    
    # ä½ çš„å¾®åšURL
    weibo_url = "https://weibo.com/ajax/statuses/buildComments?is_reload=1&id=5243449034408233&is_show_bulletin=2&is_mix=0&count=10&uid=6856915235&fetch_level=0&locale=zh-CN"
    
    print("è¯·é€‰æ‹©çˆ¬å–æ–¹å¼:")
    print("1. ä½¿ç”¨Cookieçˆ¬å–ï¼ˆéœ€è¦ç™»å½•ï¼Œèƒ½è·å–æ›´å¤šæ•°æ®ï¼‰")
    print("2. ç®€åŒ–ç‰ˆçˆ¬å–ï¼ˆæ— éœ€ç™»å½•ï¼Œä½†å¯èƒ½å—é™ï¼‰")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2): ").strip()
    
    if choice == "1":
        # å°è¯•ä»æ–‡ä»¶è¯»å–cookie
        try:
            with open('weibo_cookie.txt', 'r') as f:
                cookie = f.read()
                print(f"ä½¿ç”¨å·²æœ‰cookieï¼ˆ{len(cookie)}å­—ç¬¦ï¼‰")
                success = crawl_with_cookie(weibo_url, cookie)
                if not success:
                    print("ğŸ˜” çˆ¬å–å¤±è´¥ï¼Œå¯èƒ½éœ€è¦æ›´æ–°cookie")
                    input("æŒ‰Enteré”®æ‰‹åŠ¨è¾“å…¥æ–°cookie...")
                    crawl_with_cookie(weibo_url)  # ä¸å¸¦cookieï¼Œä¼šæç¤ºè¾“å…¥
        except:
            print("æœªæ‰¾åˆ°cookieæ–‡ä»¶ï¼Œéœ€è¦æ‰‹åŠ¨è¾“å…¥")
            crawl_with_cookie(weibo_url)
    
    elif choice == "2":
        simple_crawl(weibo_url)
    
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆ...")
        simple_crawl(weibo_url)