"""
æ•°æ®å¤„ç†æœåŠ¡ - è´Ÿè´£æ¸…æ´—ã€åˆ†è¯ã€å…³é”®è¯æå–
"""

import re
import os
import jieba
from typing import List, Dict, Tuple
from datetime import datetime
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from app import db
from app.models import Topic, WeiboPost, Keyword


class DataProcessingService:
    """æ•°æ®å¤„ç†æœåŠ¡ç±»"""
    
    def __init__(self):
        # åŠ è½½åœç”¨è¯
        self.stopwords = self._load_stopwords()
        
    def _load_stopwords(self) -> set:
        """åŠ è½½åœç”¨è¯è¡¨"""
        try:
            # è·å–stopwords.txtçš„ç»å¯¹è·¯å¾„
            current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            stopwords_path = os.path.join(current_dir, 'utils', 'stopwords.txt')
            
            if os.path.exists(stopwords_path):
                with open(stopwords_path, 'r', encoding='utf-8') as f:
                    stopwords = set(line.strip() for line in f if line.strip())
                print(f"[INFO] æˆåŠŸåŠ è½½ {len(stopwords)} ä¸ªåœç”¨è¯")
                return stopwords
            else:
                print(f"[WARNING] åœç”¨è¯æ–‡ä»¶ä¸å­˜åœ¨: {stopwords_path}ï¼Œä½¿ç”¨é»˜è®¤åœç”¨è¯")
                return {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½'}
        except Exception as e:
            print(f"[ERROR] åŠ è½½åœç”¨è¯å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤åœç”¨è¯")
            return {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½'}
    
    # ====== é˜¶æ®µä¸€ï¼šæ•°æ®è¯»å– ======
    
    def fetch_topic_posts(self, topic_id: int) -> Dict:
        """
        è¯»å–æŒ‡å®šè¯é¢˜çš„æ‰€æœ‰å¾®åšæ•°æ®
        
        Args:
            topic_id: è¯é¢˜ID
            
        Returns:
            {
                'topic_id': int,
                'topic_name': str,
                'posts': [WeiboPoståˆ—è¡¨]
            }
        """
        topic = Topic.query.get(topic_id)
        if not topic:
            return None
        
        posts = WeiboPost.query.filter_by(topic_id=topic_id).all()
        return {
            'topic_id': topic.id,
            'topic_name': topic.topic_name,
            'posts': posts
        }
    
    # ====== é˜¶æ®µäºŒï¼šæ–‡æœ¬æ¸…æ´— ======
    
    def clean_text(self, raw_text: str) -> str:
        """
        æ¸…æ´—å•æ¡æ–‡æœ¬
        
        å¤„ç†æ­¥éª¤:
        1. å»é™¤URL
        2. å»é™¤@ç”¨æˆ·å
        3. å»é™¤emojiè¡¨æƒ…
        4. å»é™¤ç‰¹æ®Šå­—ç¬¦
        5. å»é™¤å¤šä½™ç©ºæ ¼
        
        Args:
            raw_text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not raw_text:
            return ""
        
        text = raw_text
        
        # 1. å»é™¤URLé“¾æ¥
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # 2. å»é™¤@ç”¨æˆ·å
        text = re.sub(r'@[\w\u4e00-\u9fa5]+', '', text)
        
        # 3. å»é™¤è¯é¢˜æ ‡ç­¾#(å¯é€‰ï¼Œæ ¹æ®éœ€æ±‚)
        # text = re.sub(r'#[^#]+#', '', text)
        
        # 4. å»é™¤emojiå’Œç‰¹æ®Šç¬¦å·
        # TODO: æ›´å®Œå–„çš„emojiè¿‡æ»¤
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
        
        # 5. å»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    # ====== é˜¶æ®µä¸‰ï¼šåˆ†è¯å¤„ç† ======
    
    def segment_text(self, text: str) -> List[str]:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶è¿‡æ»¤
        
        Args:
            text: æ¸…æ´—åçš„æ–‡æœ¬
            
        Returns:
            åˆ†è¯åˆ—è¡¨
        """
        if not text:
            return []
        
        # 1. jiebaåˆ†è¯
        words = jieba.cut(text)
        
        # 2. è¿‡æ»¤å¤„ç†
        filtered_words = []
        for word in words:
            word = word.strip()
            # è¿‡æ»¤æ¡ä»¶ï¼š
            # - ä¸åœ¨åœç”¨è¯è¡¨ä¸­
            # - é•¿åº¦å¤§äº1
            # - ä¸æ˜¯çº¯æ•°å­—
            if (word and 
                word not in self.stopwords and 
                len(word) > 1 and 
                not word.isdigit()):
                filtered_words.append(word)
        
        return filtered_words
    
    # ====== é˜¶æ®µå››ï¼šå…³é”®è¯æå– ======
    
    def extract_keywords_tf(self, posts: List[WeiboPost], top_n: int = 50) -> List[Dict]:
        """
        ä½¿ç”¨è¯é¢‘(TF)æ–¹æ³•æå–å…³é”®è¯
        
        Args:
            posts: å¾®åšåˆ—è¡¨
            top_n: è¿”å›å‰Nä¸ªå…³é”®è¯
            
        Returns:
            [
                {'keyword': str, 'frequency': int, 'weight': float},
                ...
            ]
        """
        # 1. æ”¶é›†æ‰€æœ‰åˆ†è¯
        all_words = []
        for post in posts:
            # æ¸…æ´—æ–‡æœ¬
            cleaned = self.clean_text(post.content)
            # åˆ†è¯
            words = self.segment_text(cleaned)
            all_words.extend(words)
        
        # 2. ç»Ÿè®¡è¯é¢‘
        word_counter = Counter(all_words)
        
        # 3. è·å–Top N
        top_keywords = word_counter.most_common(top_n)
        
        # 4. æ ¼å¼åŒ–ç»“æœ
        total_words = len(all_words)
        results = []
        for keyword, freq in top_keywords:
            results.append({
                'keyword': keyword,
                'frequency': freq,
                'weight': freq / total_words if total_words > 0 else 0
            })
        
        return results
    
    def extract_keywords_tfidf(self, posts: List[WeiboPost], top_n: int = 50) -> List[Dict]:
        """
        ä½¿ç”¨TF-IDFæ–¹æ³•æå–å…³é”®è¯
        
        Args:
            posts: å¾®åšåˆ—è¡¨
            top_n: è¿”å›å‰Nä¸ªå…³é”®è¯
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        if not posts:
            return []
        
        # 1. å‡†å¤‡æ–‡æ¡£åˆ—è¡¨å’Œç»Ÿè®¡è¯é¢‘ï¼ˆæ¯æ¡å¾®åšæ˜¯ä¸€ä¸ªæ–‡æ¡£ï¼‰
        documents = []
        word_freq_counter = Counter()
        
        for post in posts:
            # æ¸…æ´—æ–‡æœ¬
            cleaned = self.clean_text(post.content)
            # åˆ†è¯
            words = self.segment_text(cleaned)
            # ç”¨ç©ºæ ¼è¿æ¥åˆ†è¯ç»“æœä½œä¸ºæ–‡æ¡£
            documents.append(' '.join(words))
            # åŒæ—¶ç»Ÿè®¡å…¨å±€è¯é¢‘
            word_freq_counter.update(words)
        
        # å¦‚æœæ–‡æ¡£ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ
        if not documents or all(not doc for doc in documents):
            return []
        
        # 2. ä½¿ç”¨TfidfVectorizerè®¡ç®—TF-IDF
        try:
            vectorizer = TfidfVectorizer(max_features=top_n * 2)  # æå–æ›´å¤šç‰¹å¾é¿å…ä¸¢å¤±
            tfidf_matrix = vectorizer.fit_transform(documents)
            
            # 3. æå–ç‰¹å¾è¯å’Œæƒé‡
            feature_names = vectorizer.get_feature_names_out()
            
            # è®¡ç®—æ¯ä¸ªè¯çš„å¹³å‡TF-IDFåˆ†æ•°ï¼ˆè·¨æ‰€æœ‰æ–‡æ¡£ï¼‰
            tfidf_scores = tfidf_matrix.sum(axis=0).A1
            
            # 4. ä¸ºæ¯ä¸ªè¯åˆ›å»º(è¯, TF-IDFåˆ†æ•°, è¯é¢‘)çš„å…ƒç»„
            keyword_data = []
            for idx, word in enumerate(feature_names):
                keyword_data.append({
                    'keyword': word,
                    'tfidf_score': float(tfidf_scores[idx]),
                    'frequency': word_freq_counter.get(word, 0)
                })
            
            # 5. æŒ‰TF-IDFåˆ†æ•°æ’åºå¹¶å–Top N
            keyword_data.sort(key=lambda x: x['tfidf_score'], reverse=True)
            top_keywords = keyword_data[:top_n]
            
            # 6. æ ¼å¼åŒ–ç»“æœï¼ˆä¿ç•™é¢‘ç‡å’Œæƒé‡ï¼‰
            results = []
            for kw in top_keywords:
                results.append({
                    'keyword': kw['keyword'],
                    'frequency': kw['frequency'],
                    'weight': kw['tfidf_score']
                })
            
            return results
            
        except Exception as e:
            print(f"[ERROR] TF-IDFæå–å¤±è´¥: {e}")
            # é™çº§åˆ°TFæ–¹æ³•
            print("[INFO] é™çº§ä½¿ç”¨TFæ–¹æ³•")
            return self.extract_keywords_tf(posts, top_n)
    
    # ====== é˜¶æ®µäº”ï¼šæ•°æ®ä¿å­˜ ======
    
    def save_keywords(self, topic_id: int, keywords: List[Dict], time_period: str = None) -> bool:
        """
        ä¿å­˜å…³é”®è¯åˆ°æ•°æ®åº“
        
        Args:
            topic_id: è¯é¢˜ID
            keywords: å…³é”®è¯åˆ—è¡¨ [{'keyword': str, 'frequency': int}, ...]
            time_period: æ—¶é—´æ®µæ ‡è¯†ï¼Œå¦‚ '2024-12-16'
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # 1. å‡†å¤‡æ—¶é—´æ®µ
            if not time_period:
                time_period = datetime.now().strftime('%Y-%m-%d')
            
            # 2. åˆ é™¤è¯¥è¯é¢˜è¯¥æ—¶é—´æ®µçš„æ—§å…³é”®è¯ï¼ˆé¿å…é‡å¤ï¼‰
            deleted_count = Keyword.query.filter_by(
                topic_id=topic_id, 
                time_period=time_period
            ).delete()
            if deleted_count > 0:
                print(f"[INFO] åˆ é™¤äº† {deleted_count} ä¸ªæ—§å…³é”®è¯è®°å½•")
            
            # 3. å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
            keyword_objs = []
            for kw in keywords:
                keyword_objs.append({
                    'topic_id': topic_id,
                    'keyword': kw['keyword'],
                    'frequency': kw.get('frequency', 0),
                    'time_period': time_period,
                    'analyzed_at': datetime.utcnow()
                })
            
            # 4. æ‰¹é‡æ’å…¥
            if keyword_objs:
                db.session.bulk_insert_mappings(Keyword, keyword_objs)
                db.session.commit()
                print(f"[INFO] æˆåŠŸä¿å­˜ {len(keyword_objs)} ä¸ªå…³é”®è¯åˆ°æ•°æ®åº“")
            
            return True
            
        except Exception as e:
            db.session.rollback()
            print(f"[ERROR] ä¿å­˜å…³é”®è¯å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    # ====== ä¸»å¤„ç†å‡½æ•° ======
    
    def process_topic(self, topic_id: int, method: str = 'tf', top_n: int = 50) -> Dict:
        """
        å¤„ç†å•ä¸ªè¯é¢˜çš„å®Œæ•´æµç¨‹
        
        Args:
            topic_id: è¯é¢˜ID
            method: å…³é”®è¯æå–æ–¹æ³• 'tf' æˆ– 'tfidf'
            top_n: æå–å‰Nä¸ªå…³é”®è¯
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        start_time = datetime.now()
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹å¤„ç†è¯é¢˜ ID: {topic_id}")
        print(f"{'='*60}\n")
        
        # 1. è¯»å–æ•°æ®
        print("[1/4] è¯»å–æ•°æ®...")
        topic_data = self.fetch_topic_posts(topic_id)
        if not topic_data:
            return {'status': 'error', 'message': 'è¯é¢˜ä¸å­˜åœ¨'}
        
        posts = topic_data['posts']
        print(f"      è¯»å–åˆ° {len(posts)} æ¡å¾®åš")
        
        # 2. æ•°æ®æ¸…æ´—ï¼ˆåœ¨åˆ†è¯æ—¶å¤„ç†ï¼‰
        print("[2/4] æ•°æ®æ¸…æ´—å’Œåˆ†è¯...")
        
        # 3. æå–å…³é”®è¯
        print("[3/4] æå–å…³é”®è¯...")
        if method == 'tf':
            keywords = self.extract_keywords_tf(posts, top_n)
        elif method == 'tfidf':
            keywords = self.extract_keywords_tfidf(posts, top_n)
        else:
            return {'status': 'error', 'message': 'ä¸æ”¯æŒçš„æå–æ–¹æ³•'}
        
        print(f"      æå–åˆ° {len(keywords)} ä¸ªå…³é”®è¯")
        
        # 4. ä¿å­˜ç»“æœ
        print("[4/4] ä¿å­˜å…³é”®è¯...")
        success = self.save_keywords(topic_id, keywords)
        
        # 5. ç»Ÿè®¡ä¿¡æ¯
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        result = {
            'status': 'success' if success else 'error',
            'topic_id': topic_id,
            'topic_name': topic_data['topic_name'],
            'processed_posts': len(posts),
            'keywords_count': len(keywords),
            'top_10_keywords': keywords[:10],
            'processing_time': f"{processing_time:.2f}s"
        }
        
        print(f"\n{'='*60}")
        print(f"å¤„ç†å®Œæˆï¼")
        print(f"è€—æ—¶: {result['processing_time']}")
        print(f"Top 10 å…³é”®è¯:")
        for i, kw in enumerate(result['top_10_keywords'], 1):
            print(f"  {i}. {kw['keyword']}: {kw['frequency']} æ¬¡")
        print(f"{'='*60}\n")
        
        return result
    
    def process_all_topics(self, method: str = 'tf', top_n: int = 50) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†æ‰€æœ‰æ´»è·ƒè¯é¢˜
        
        Args:
            method: å…³é”®è¯æå–æ–¹æ³•
            top_n: æ¯ä¸ªè¯é¢˜æå–çš„å…³é”®è¯æ•°é‡
            
        Returns:
            æ‰€æœ‰è¯é¢˜çš„å¤„ç†ç»“æœåˆ—è¡¨
        """
        # 1. æŸ¥è¯¢æ‰€æœ‰æ´»è·ƒè¯é¢˜
        active_topics = Topic.query.filter_by(is_active=True).all()
        print(f"\n[INFO] æ‰¾åˆ° {len(active_topics)} ä¸ªæ´»è·ƒè¯é¢˜\n")
        
        # 2. éå†å¤„ç†
        results = []
        for i, topic in enumerate(active_topics, 1):
            print(f"\nå¤„ç†è¿›åº¦: {i}/{len(active_topics)}")
            try:
                result = self.process_topic(topic.id, method, top_n)
                results.append(result)
            except Exception as e:
                print(f"[ERROR] å¤„ç†è¯é¢˜ {topic.id} å¤±è´¥: {e}")
                results.append({
                    'status': 'error',
                    'topic_id': topic.id,
                    'message': str(e)
                })
        
        return results


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # åˆ›å»ºæœåŠ¡å®ä¾‹
    service = DataProcessingService()
    
    # æµ‹è¯•æ–‡æœ¬æ¸…æ´—
    test_text = "ä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼http://example.com @å¼ ä¸‰ #å¾®åšè¯é¢˜# ğŸ˜Š"
    cleaned = service.clean_text(test_text)
    print(f"åŸæ–‡: {test_text}")
    print(f"æ¸…æ´—å: {cleaned}")
    
    # æµ‹è¯•åˆ†è¯
    words = service.segment_text(cleaned)
    print(f"åˆ†è¯ç»“æœ: {words}")
    
    # TODO: æµ‹è¯•å®Œæ•´æµç¨‹ï¼ˆéœ€è¦æ•°æ®åº“æ•°æ®ï¼‰
    # result = service.process_topic(topic_id=1, method='tf', top_n=50)
    # print(result)
