# æ•°æ®å¤„ç†æ¨¡å— - å¿«é€Ÿå¼€å§‹

## ğŸ“ å·²åˆ›å»ºçš„æ–‡ä»¶

æˆ‘å·²ç»ä¸ºä½ å‡†å¤‡å¥½äº†æ•°æ®å¤„ç†æ¨¡å—çš„å®Œæ•´æ¡†æ¶ï¼š

### 1. è®¾è®¡æ–‡æ¡£
- **`DATA_PROCESSING_PLAN.md`** - è¯¦ç»†çš„æ•°æ®å¤„ç†æµç¨‹è®¾è®¡æ–¹æ¡ˆ
  - åŒ…å«å®Œæ•´çš„5é˜¶æ®µå¤„ç†æµç¨‹
  - ä»£ç ç»“æ„è§„åˆ’
  - æŠ€æœ¯é€‰å‹å»ºè®®
  - å®ç°ä¼˜å…ˆçº§

### 2. æ ¸å¿ƒä»£ç 
- **`app/services/data_processing_service.py`** - æ•°æ®å¤„ç†æœåŠ¡ç±»
  - âœ… `clean_text()` - æ–‡æœ¬æ¸…æ´—ï¼ˆå»URLã€@ç”¨æˆ·åã€emojiç­‰ï¼‰
  - âœ… `segment_text()` - ä¸­æ–‡åˆ†è¯ï¼ˆjieba + åœç”¨è¯è¿‡æ»¤ï¼‰
  - âœ… `extract_keywords_tf()` - è¯é¢‘ç»Ÿè®¡æå–å…³é”®è¯
  - ğŸ”§ `extract_keywords_tfidf()` - TF-IDFæå–ï¼ˆå¾…å®ç°ï¼‰
  - ğŸ”§ `fetch_topic_posts()` - æ•°æ®åº“æŸ¥è¯¢ï¼ˆå¾…å®ç°ï¼‰
  - ğŸ”§ `save_keywords()` - ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¾…å®ç°ï¼‰
  - âœ… `process_topic()` - å®Œæ•´å¤„ç†æµç¨‹

### 3. é…ç½®æ–‡ä»¶
- **`app/utils/stopwords.txt`** - ä¸­æ–‡åœç”¨è¯è¡¨

### 4. æµ‹è¯•è„šæœ¬
- **`test_data_processing.py`** - æµ‹è¯•è„šæœ¬ï¼ˆ3ç§æ¨¡å¼ï¼‰

---

## ğŸš€ å¿«é€Ÿæµ‹è¯•

### æ¨¡å¼1: æµ‹è¯•åŸºç¡€åŠŸèƒ½ï¼ˆæ–‡æœ¬æ¸…æ´—å’Œåˆ†è¯ï¼‰

```bash
cd backend
.\venv\Scripts\python.exe test_data_processing.py --mode basic
```

**é¢„æœŸè¾“å‡ºï¼š**
- å±•ç¤ºæ–‡æœ¬æ¸…æ´—æ•ˆæœï¼ˆå»é™¤URLã€@ã€emojiï¼‰
- å±•ç¤ºåˆ†è¯ç»“æœï¼ˆè¿‡æ»¤åœç”¨è¯ï¼‰

### æ¨¡å¼2: åˆ›å»ºæµ‹è¯•æ•°æ®

```bash
.\venv\Scripts\python.exe test_data_processing.py --mode create_sample
```

**åŠŸèƒ½ï¼š**
- åœ¨æ•°æ®åº“ä¸­åˆ›å»ºæµ‹è¯•è¯é¢˜ "#æ•°æ®å¤„ç†æµ‹è¯•#"
- æ’å…¥10æ¡ç¤ºä¾‹å¾®åšæ•°æ®

### æ¨¡å¼3: æµ‹è¯•æ•°æ®åº“å¤„ç†

```bash
.\venv\Scripts\python.exe test_data_processing.py --mode database
```

**åŠŸèƒ½ï¼š**
- æŸ¥çœ‹æ•°æ®åº“ä¸­çš„è¯é¢˜å’Œå¾®åšç»Ÿè®¡
- æ˜¾ç¤ºç¬¬ä¸€ä¸ªè¯é¢˜çš„å‰5æ¡å¾®åš

---

## ğŸ“‹ å¾…å®Œæˆçš„TODO

### ä¼˜å…ˆçº§1: æ ¸å¿ƒåŠŸèƒ½

åœ¨ `app/services/data_processing_service.py` ä¸­å®Œæˆä»¥ä¸‹å‡½æ•°ï¼š

1. **`fetch_topic_posts()`** - æ•°æ®åº“æŸ¥è¯¢
   ```python
   def fetch_topic_posts(self, topic_id: int):
       topic = Topic.query.get(topic_id)
       if not topic:
           return None
       posts = WeiboPost.query.filter_by(topic_id=topic_id).all()
       return {
           'topic_id': topic.id,
           'topic_name': topic.topic_name,
           'posts': posts
       }
   ```

2. **`save_keywords()`** - ä¿å­˜å…³é”®è¯
   ```python
   def save_keywords(self, topic_id: int, keywords: List[Dict], time_period: str = None):
       try:
           if not time_period:
               time_period = datetime.now().strftime('%Y-%m-%d')
           
           # åˆ é™¤æ—§æ•°æ®ï¼ˆå¯é€‰ï¼‰
           Keyword.query.filter_by(topic_id=topic_id, time_period=time_period).delete()
           
           # æ‰¹é‡æ’å…¥
           keyword_objs = []
           for kw in keywords:
               keyword_objs.append({
                   'topic_id': topic_id,
                   'keyword': kw['keyword'],
                   'frequency': kw['frequency'],
                   'time_period': time_period,
                   'analyzed_at': datetime.utcnow()
               })
           
           db.session.bulk_insert_mappings(Keyword, keyword_objs)
           db.session.commit()
           return True
       except Exception as e:
           db.session.rollback()
           print(f"Error: {e}")
           return False
   ```

### ä¼˜å…ˆçº§2: å¢å¼ºåŠŸèƒ½

3. **`extract_keywords_tfidf()`** - TF-IDF
   ```python
   from sklearn.feature_extraction.text import TfidfVectorizer
   
   def extract_keywords_tfidf(self, posts, top_n=50):
       # å‡†å¤‡æ–‡æ¡£
       documents = []
       for post in posts:
           cleaned = self.clean_text(post.content)
           words = self.segment_text(cleaned)
           documents.append(' '.join(words))
       
       # TF-IDF
       vectorizer = TfidfVectorizer(max_features=top_n)
       tfidf_matrix = vectorizer.fit_transform(documents)
       feature_names = vectorizer.get_feature_names_out()
       
       # è®¡ç®—æ€»åˆ†æ•°
       scores = tfidf_matrix.sum(axis=0).A1
       keyword_scores = list(zip(feature_names, scores))
       keyword_scores.sort(key=lambda x: x[1], reverse=True)
       
       # æ ¼å¼åŒ–ç»“æœ
       results = []
       for keyword, score in keyword_scores[:top_n]:
           results.append({
               'keyword': keyword,
               'frequency': 0,  # å¯ä»¥å•ç‹¬ç»Ÿè®¡
               'weight': float(score)
           })
       return results
   ```

---

## ğŸ§ª å®Œæ•´æµ‹è¯•æµç¨‹

### æ­¥éª¤1: æµ‹è¯•åŸºç¡€åŠŸèƒ½
```bash
.\venv\Scripts\python.exe test_data_processing.py --mode basic
```

### æ­¥éª¤2: åˆ›å»ºæµ‹è¯•æ•°æ®
```bash
.\venv\Scripts\python.exe test_data_processing.py --mode create_sample
```

### æ­¥éª¤3: å®Œå–„ä»£ç 
- åœ¨ `data_processing_service.py` ä¸­å®ç°ä¸Šè¿°TODO
- éœ€è¦å–æ¶ˆæ³¨é‡Šç›¸å…³æ•°æ®åº“æ“ä½œä»£ç 

### æ­¥éª¤4: è¿è¡Œå®Œæ•´å¤„ç†
```python
# åœ¨Pythonç¯å¢ƒä¸­æµ‹è¯•
from app import create_app
from app.services.data_processing_service import DataProcessingService

app = create_app()
with app.app_context():
    service = DataProcessingService()
    result = service.process_topic(topic_id=1, method='tf', top_n=50)
    print(result)
```

### æ­¥éª¤5: æŸ¥çœ‹ç»“æœ
```python
from app.models import Keyword

with app.app_context():
    # æŸ¥è¯¢æŸè¯é¢˜çš„å…³é”®è¯
    keywords = Keyword.query.filter_by(topic_id=1).order_by(Keyword.frequency.desc()).limit(20).all()
    for kw in keywords:
        print(f"{kw.keyword}: {kw.frequency}")
```

---

## ğŸ“¦ ä¾èµ–æ£€æŸ¥

ç¡®ä¿ä»¥ä¸‹PythonåŒ…å·²å®‰è£…ï¼š

```bash
pip install jieba
pip install scikit-learn  # ç”¨äºTF-IDF
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®

### ç«‹å³å¯åšï¼š
1. âœ… è¿è¡ŒåŸºç¡€åŠŸèƒ½æµ‹è¯•ï¼ˆä¸éœ€è¦æ•°æ®åº“ï¼‰
2. âœ… æŸ¥çœ‹æ–‡æœ¬æ¸…æ´—å’Œåˆ†è¯æ•ˆæœ

### éœ€è¦æ•°æ®åï¼š
3. ğŸ”§ å®ç° `fetch_topic_posts()` å’Œ `save_keywords()`
4. ğŸ”§ è¿è¡Œå®Œæ•´æµç¨‹æµ‹è¯•
5. ğŸ”§ éªŒè¯å…³é”®è¯ä¿å­˜åˆ°æ•°æ®åº“

### é«˜çº§ä¼˜åŒ–ï¼š
6. â­ å®ç°TF-IDFç®—æ³•
7. â­ æ·»åŠ æ—¶é—´ç»´åº¦åˆ†æ
8. â­ åˆ›å»ºAPIæ¥å£ï¼ˆå¯é€‰ï¼‰

---

## ğŸ“– å‚è€ƒæ–‡æ¡£

- **è®¾è®¡æ–¹æ¡ˆ**: `DATA_PROCESSING_PLAN.md` - å®Œæ•´æ€è·¯å’Œæ¶æ„
- **ä»£ç æ¡†æ¶**: `app/services/data_processing_service.py` - å¯ç›´æ¥ä½¿ç”¨çš„ç±»
- **æµ‹è¯•è„šæœ¬**: `test_data_processing.py` - 3ç§æµ‹è¯•æ¨¡å¼

---

## ğŸ’¡ æç¤º

1. **æ¸è¿›å¼å¼€å‘**: å…ˆè·‘é€šåŸºç¡€åŠŸèƒ½ â†’ å†è¿æ¥æ•°æ®åº“ â†’ æœ€åä¼˜åŒ–ç®—æ³•
2. **å°æ­¥æµ‹è¯•**: æ¯å®Œæˆä¸€ä¸ªå‡½æ•°å°±ç«‹å³æµ‹è¯•
3. **æŸ¥çœ‹æ—¥å¿—**: ä»£ç ä¸­æœ‰ä¸°å¯Œçš„printè¾“å‡ºï¼Œæ–¹ä¾¿è°ƒè¯•
4. **æ‰©å±•åœç”¨è¯**: `stopwords.txt` å¯ä»¥ç»§ç»­æ·»åŠ æ›´å¤šè¯æ±‡

---

**å¼€å§‹æµ‹è¯•å§ï¼** ğŸš€

```bash
cd backend
.\venv\Scripts\python.exe test_data_processing.py --mode basic
```
